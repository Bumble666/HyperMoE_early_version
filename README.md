# HyperMoE_early_version

The official implementation of the paper "HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts"

**Our code will be released soon.**

## Weight

We provide model weights tuned on SQuAD :

[Switch Transformer](https://drive.google.com/file/d/1B_zwvAC6RrrCo8g72O6yu_HOvTVhPgCm/view?usp=sharing)

[Switch Transformer HyperMoE](https://drive.google.com/file/d/10JC1FQLrl4KeQmpPDYVR_aRmRal0TqJF/view?usp=sharing)

## Citation

if you find this repository useful, please cite our paper:

    @article{hao2024hmoe,
      title={HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts},
      author={Hao Zhao, Zihan Qiu, Huijia Wu, Zili Wang, Zhaofeng He, Jie Fu},
      journal={arXiv preprint},
      year={2024}
    }
