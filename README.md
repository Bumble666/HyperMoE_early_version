# HyperMoE_early_version

The official implementation of the paper "HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts"

**Our code will be released soon.**

## Weight

We provide model weights trained on SQuAD :

[Switch Transformer](https://drive.google.com/drive/u/0/my-drive)

[Switch Transformer HyperMoE](https://drive.google.com/drive/u/0/my-drive)

if you find this repository useful, please cite our paper:

    @article{hao2024hmoe,
      title={HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts},
      author={Hao Zhao, Zihan Qiu, Huijia Wu, Zili Wang, Zhaofeng He, Jie Fu},
      journal={arXiv preprint},
      year={2024}
    }
